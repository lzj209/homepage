---
abstract: >
  We propose a universal multimodal transformer and introduce the multi-task
  learning method to learn joint representations among different modalities as
  well as generate informative and fluent responses by leveraging the
  pre-trained language model. 
slides: ""
url_pdf: ""
publication_types:
  - "2"
  - "1"
authors:
  - Zekang Li
  - Zongjia Li
  - Jinchao Zhang
  - Yang Feng
  - Jie Zhou
author_notes: []
publication: IEEE/ACM Transactions on Audio, Speech, and Language Processing
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: "Bridging text and video: A universal multimodal transformer for
  video-audio scene-aware dialog"
doi: ""
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
date: 2021-03-12T00:00:00.000Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""
---

{{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}}

Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).
