---
title: "Bridging text and video: A universal multimodal transformer for
  video-audio scene-aware dialog"
publication_types:
  - "2"
  - "1"
authors:
  - Zekang Li
  - Zongjia Li
  - Jinchao Zhang
  - Yang Feng
  - Jie Zhou
publication_short: ""
abstract: "Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate
  responses when chatting about a given video, which is organized as a track of
  the Dialog System Technology Challenge (DSTC8). There are two challenges in
  this task: 1) making effective interaction among different modalities; 2)
  better understanding dialogues and generating informative responses. To tackle
  the challenges, we propose a universal multimodal transformer and introduce
  the multi-task learning method to learn joint representations among different
  modalities as well as generate informative and fluent responses by leveraging
  the pre-trained language model. Our method extends the natural language
  generation pre-trained model to multimodal dialogue generation task, which
  allows fine-tuning language models to capture information across both visual
  and textual modalities. Our system achieves the best performance in the
  objective evaluation in both DSTC7-AVSD and DSTC8-AVSD dataset and achieves an
  impressive 98.4% of the human performance based on human ratings in the
  DSTC8-AVSD challenge."
draft: false
featured: false
tags: []
slides: ""
url_pdf: "https://ieeexplore.ieee.org/abstract/document/9376902"
image:
  caption: ""
  focal_point: ""
  preview_only: true
summary: ""
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
author_notes: []
doi: ""
publication: IEEE/ACM Transactions on Audio, Speech, and Language Processing
projects: []
date: 2021-03-12T00:00:00.000Z
url_slides: ""
publishDate: 2021-03-12T00:00:00.000Z
url_poster: ""
url_code: "https://github.com/ictnlp/DSTC8-AVSD"
---
